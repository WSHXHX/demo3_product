# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html

# useful for handling different item types with a single interface
import json
import urllib3

import pymysql
import psycopg2
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

from scrapy.exceptions import NotConfigured

from itemadapter import ItemAdapter

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class Demo3ProductPipeline:
    def process_item(self, item, spider):
        return item


class CheckExistPipeline:
    def __init__(self, es_host, es_user, es_pass, index_name):
        self.es_host = es_host
        self.es_user = es_user
        self.es_pass = es_pass
        self.index_name = index_name
        self.es = None

    @classmethod
    def from_crawler(cls, crawler):
        es_host = crawler.settings.get("ES_HOST")
        es_user = crawler.settings.get("ES_USER")
        es_pass = crawler.settings.get("ES_PASS")
        index_name = crawler.settings.get("INDEX_NAME")

        if not all([es_host, es_user, es_pass, index_name]):
            raise NotConfigured("Elasticsearch settings are incomplete")

        return cls(es_host, es_user, es_pass, index_name)

    def open_spider(self, spider):
        """è¿æ¥ Elasticsearch"""
        self.es = Elasticsearch(
            [self.es_host],
            basic_auth=(self.es_user, self.es_pass),
            verify_certs=False,
            ssl_show_warn=False,
        )

    def process_item(self, item, spider):
        """æ£€æŸ¥ item æ˜¯å¦å­˜åœ¨äº ES"""
        item_id = item.get("mysqlid")

        if not item_id:
            return item    # item æ²¡æœ‰ id ä¸æ£€æŸ¥

        try:
            # ES æŸ¥è¯¢æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            exists = self.es.exists(index=self.index_name, id=item_id)
        except Exception as e:
            spider.logger.error(f"ES exists check failed: {e}")
            exists = False

        # è®¾ç½® item['update']
        if exists:
            item["update"] = True
        else:
            item["update"] = False

        return item


class MySQLPipeline:
    def __init__(self, host, port, user, password, database):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            host=crawler.settings.get("MYSQL_HOST"),
            port=crawler.settings.getint("MYSQL_PORT"),
            user=crawler.settings.get("MYSQL_USER"),
            password=crawler.settings.get("MYSQL_PASSWORD"),
            database=crawler.settings.get("MYSQL_DB"),
        )

    def open_spider(self, spider):
        self.conn = pymysql.connect(
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.password,
            database=self.database,
            charset="utf8mb4"
        )
        self.cursor = self.conn.cursor()

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()

    def process_item(self, item, spider):
        data = ItemAdapter(item).asdict()

        # å¦‚æœéœ€è¦æ›´æ–°
        if data.get("update") is True:
            update_sql = """
            UPDATE collection_products
            SET 
                title=%s, handle=%s, description=%s, vendor=%s, category=%s,
                original_price=%s, current_price=%s, images=%s, variants=%s,
                tags=%s, updated_at=%s, type=%s, platform=%s, options=%s
            WHERE id=%s
            """

            try:
                self.cursor.execute(update_sql, (
                    data.get("title"),
                    data.get("handle"),
                    data.get("description"),
                    data.get("vendor"),
                    data.get("category"),
                    data.get("original_price"),
                    data.get("current_price"),
                    data.get("images"),
                    data.get("variants"),
                    data.get("tags"),
                    data.get("updated_at"),
                    data.get("type"),
                    data.get("platform"),
                    data.get("options"),
                    data["mysqlid"]
                ))
                self.conn.commit()

                spider.logger.info(f"ğŸ”„ Updated MySQL id {data['mysqlid']} ({data.get('title')})")

            except Exception as e:
                spider.logger.error(f"Update error: {e}")
                self.conn.rollback()

            return item

        # å¦åˆ™æ’å…¥
        sql = """
        INSERT INTO collection_products (task_id, user_id, cid, domain, title, handle, description, vendor, category, original_price, current_price, images, variants, tags, created_at, updated_at, type, platform, options)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        try:
            self.cursor.execute(sql, (
                data.get("task_id"),
                data.get("user_id"),
                data.get("cid"),
                data.get("domain"),
                data.get("title"),
                data.get("handle"),
                data.get("description"),
                data.get("vendor"),
                data.get("category"),
                data.get("original_price"),
                data.get("current_price"),
                data.get("images"),
                data.get("variants"),
                data.get("tags"),
                data.get("created_at"),
                data.get("updated_at"),
                data.get("type"),
                data.get("platform"),
                data.get("options"),
            ))
            self.conn.commit()

            # è·å–æ’å…¥çš„ä¸»é”® ID
            handle = data.get("handle")
            nnow = data.get("created_at")

            self.cursor.execute(
                "SELECT id FROM collection_products WHERE handle=%s AND created_at=%s",
                (handle, nnow)
            )
            ids = self.cursor.fetchone()
            item["mysqlid"] = ids[0]

            spider.logger.info(f"ğŸ¬ Insert MySQL id {item['mysqlid']} ({data.get('title')})")

        except Exception as e:
            spider.logger.error(f"Insert error: {e}")
            self.conn.rollback()

        return item


class ElasticsearchPipeline:
    def __init__(self, es_host, es_user, es_pass, index_name):
        self.es_host = es_host
        self.es_user = es_user
        self.es_pass = es_pass
        self.index_name = index_name
        self.es = None

    @classmethod
    def from_crawler(cls, crawler):
        es_host = crawler.settings.get("ES_HOST")
        es_user = crawler.settings.get("ES_USER")
        es_pass = crawler.settings.get("ES_PASS")
        index_name = crawler.settings.get("INDEX_NAME")

        if not all([es_host, es_user, es_pass, index_name]):
            raise NotConfigured("Elasticsearch settings are incomplete")

        return cls(es_host, es_user, es_pass, index_name)

    def open_spider(self, spider):
        self.es = Elasticsearch(
            [self.es_host],
            basic_auth=(self.es_user, self.es_pass),
            verify_certs=False,
            ssl_show_warn=False,
        )

    def process_item(self, item, spider):

        doc = {
            "id": item["mysqlid"],
            "task_id": item["task_id"],
            "user_id": item["user_id"],
            "cid": item["cid"],
            "domain": item["domain"],
            "title": item["title"],
            "handle": item["handle"],
            "vendor": item["vendor"],
            "category": ",".join(json.loads(item["category"])),
            "original_price": item["original_price"],
            "current_price": item["current_price"],
            "tags": ",".join(json.loads(item["tags"])),
            "created_at": item["created_at"],
            "updated_at": item["updated_at"],
            "type": item["type"],
            "platform": item["platform"]
        }

        # æ›´æ–°æ¨¡å¼
        if item.get("update") is True:
            try:
                self.es.update(
                    index=self.index_name,
                    id=doc["id"],
                    doc={"doc": doc},
                    doc_as_upsert=True  # å¦‚æœä¸å­˜åœ¨å°±åˆ›å»º
                )
                spider.logger.info(f"ğŸ”„ Updated ES id {doc['id']} ({doc['title']})")
            except Exception as e:
                spider.logger.error(f"ES update error: {e}")

            return item

        # æ’å…¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        try:
            self.es.index(index=self.index_name, id=doc["id"], document=doc)
            spider.logger.info(f"ğŸ†• Insert ES id {doc['id']} ({doc['title']})")
        except Exception as e:
            spider.logger.error(f"ES insert error: {e}")

        return item


class PostgresUpdatePipeline:
    def __init__(self, host, dbname, user, password):
        self.host = host
        self.dbname = dbname
        self.user = user
        self.password = password
        self.conn = None
        self.cursor = None

    @classmethod
    def from_crawler(cls, crawler):
        host = crawler.settings.get("POSTGRES_HOST")
        dbname = crawler.settings.get("POSTGRES_DBNAME")
        user = crawler.settings.get("POSTGRES_USER")
        password = crawler.settings.get("POSTGRES_PASSWORD")

        if not all([host, dbname, user, password]):
            raise NotConfigured("PostgreSQL settings are incomplete")

        return cls(host, dbname, user, password)

    def open_spider(self, spider):
        """åœ¨çˆ¬è™«å¯åŠ¨æ—¶è¿æ¥æ•°æ®åº“"""
        self.conn = psycopg2.connect(
            host=self.host,
            dbname=self.dbname,
            user=self.user,
            password=self.password,
        )
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        """æ ¹æ® item['postid'] æ›´æ–°çŠ¶æ€"""
        postid = item.get("postid")
        if postid:
            try:
                self.cursor.execute(
                    "UPDATE public.spider_temp SET status = 2 WHERE id = %s;",
                    (postid,)
                )
                self.conn.commit()
                spider.logger.info(f"âœ… Postgres æ›´æ–°æˆåŠŸ: id={postid}")
            except Exception as e:
                self.conn.rollback()
                spider.logger.error(f"âŒ Postgres æ›´æ–°å¤±è´¥: id={postid}, error={e}")
        else:
            spider.logger.warning("âš ï¸ Item æ²¡æœ‰ postidï¼Œè·³è¿‡ Postgres æ›´æ–°")

        return item  # ä¸€å®šè¦è¿”å› item è®©åç»­ç®¡é“èƒ½ç»§ç»­ç”¨

    def close_spider(self, spider):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()


class UpdateImagesPipline:
    def __init__(self, host, port, user, password, database):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            host=crawler.settings.get("MYSQL_HOST"),
            port=crawler.settings.getint("MYSQL_PORT"),
            user=crawler.settings.get("MYSQL_USER"),
            password=crawler.settings.get("MYSQL_PASSWORD"),
            database=crawler.settings.get("MYSQL_DB"),
        )

    def open_spider(self, spider):
        # è¿æ¥æ•°æ®åº“
        self.conn = pymysql.connect(
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.password,
            database=self.database,
            charset="utf8mb4"
        )
        self.cursor = self.conn.cursor()

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()

    def process_item(self, item, spider):

        data = ItemAdapter(item).asdict()

        # SQL æ›´æ–°è¯­å¥
        sql = """
        UPDATE collection_products
        SET images = %s
        WHERE handle = %s
        """

        try:
            # æ‰§è¡Œæ›´æ–°
            self.cursor.execute(sql, (
                data.get("images"),  # è¦æ›´æ–°çš„å†…å®¹
                data.get("handle"),  # æ¡ä»¶å­—æ®µ
            ))
            self.conn.commit()

            spider.logger.info(f'âœ… Updated images for handle: {data.get("handle")}')
        except Exception as e:
            spider.logger.error(f"Update error: {e}")
            self.conn.rollback()

        return item


class UpdateTaskTableProductNumber:

    def __init__(self, host, port, user, password, database):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            host=crawler.settings.get("MYSQL_HOST"),
            port=crawler.settings.getint("MYSQL_PORT"),
            user=crawler.settings.get("MYSQL_USER"),
            password=crawler.settings.get("MYSQL_PASSWORD"),
            database=crawler.settings.get("MYSQL_DB"),
        )

    def process_item(self, item, spider):
        return item

    def close_spider(self, spider):
        conn = pymysql.connect(
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.password,
            database=self.database,
            charset="utf8mb4"
        )
        cursor = conn.cursor()
        cursor.execute("""UPDATE collection_bath_tasks
                                SET 
                                  quantity = (SELECT COUNT(*) FROM collection_products WHERE task_id = %s),
                                  lowest_price = (SELECT MIN(original_price) FROM collection_products WHERE task_id = %s),
                                  highest_price = (SELECT MAX(original_price) FROM collection_products WHERE task_id = %s)
                                WHERE rid = %s""",
                       (spider.task_id, spider.task_id, spider.task_id, spider.task_id))
        conn.commit()
        cursor.close()
        conn.close()